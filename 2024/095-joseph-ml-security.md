# Joseph Lucas:  What is Machine Learning Security Anyway?

## Upcoming Events
Join our Meetup group for more events!
https://www.meetup.com/data-umbrella

## Key Links
- Transcript: https://github.com/data-umbrella/event-transcripts/blob/main/2024/95-joseph-ml-security.md
- Meetup Event: https://www.meetup.com/data-umbrella/events/297749677/
- Video: https://youtu.be/MEFRie7hmpM
- GitHub repo:
- Transcriber:  ? [needs a transcriber]

## Resources
- Slides: https://github.com/data-umbrella/event-transcripts/blob/main/resources/machine-learning-security.pdf
- DEFCON: https://aivillage.org/
- Securing LLM Systems Against Prompt Injection: https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/

## About the Event
Securing machine learning development and products is increasingly relevant and a growing career field. Joe Lucas, a member of NVIDIA's AI Red Team, will be introducing the field of machine learning security to help attendees secure their own work and understand how to engage with this new domain of security.

```
## Timestamps 
00:00 Data Umbrella Introduction
03:40 Speaker Introduction
04:44 Joe begins talk
05:23 Overview and Agenda
05:37 Extremely brief terminology recap
07:04 Confidentiality, integrity, and availability; software security rules and principles still apply
09:32 Threat model overview (definition and examples)
10:45 What’s new in ML security? Why are we here? Examples of new headaches
11:54 “Patching” ML models
13:29 Returning to basics (security best practices)
14:10 Three rules - GIGO, don’t anthropomorphize, model context matters
15:41 50,000-foot overview of AI security academic history
16:50 ML security canonical example - adversarial evasion
21:07 Other attack examples - model inversion, membership inference, model extraction, training data poisoning
26:24 Assessing risk from ML in practice
27:40 What is the model for?
28:47 Training data matters
30:45 Data-specific things to consider
31:46 Conclusion - take ownership of the security of your product
32:52 Learn more
35:00 Q&A begins - is the speaker related to the research by NVIDIA on securing LLM systems against comp injection log?
35:48 Q&A - What is the overlap between ML security and AI ethics?
37:27 Q&A - What is more trustworthy? Open-source models or closed-source models?
39:00 Q&A - What do you see in the future development of AI in 2024?
40:01 Q&A - How does one get into AI security?
41:18 Q&A - How long before we start seeing job titles with ML and AI security in them?
42:02 Q&A - Which is the best virtual online platform to train a model from since GPUs are expensive?
42:50 Q&A - How do we enforce security measures against some of the attacks discussed?
44:03 Linkage to Overview of Information Security presentation
```
https://github.com/data-umbrella/event-transcripts/issues/92

## About the Speaker
Joe is a senior offensive security researcher focused on AI at NVIDIA. He is the founder and chair of the NumFOCUS Security Committee and is a member of the Jupyter Security Council. He was one of the architects and hosts of the DEF CON 30 AI Village Capture the Flag competition and is passionate about machine learning security education. He served in the US Army at US Cyber Command and the 101st Airborne Division. He holds a master's degree in Computer Science from Georgia Institute of Technology and a bachelor's degree in Mathematics from the United States Military Academy. His first open source contribution was as part of a Data Umbrella sprint!

- GitHub: https://github.com/JosephTLucas
- X: https://twitter.com/josephtlucas
- LinkedIn: https://www.linkedin.com/in/josephtlucas/

#machinelearning #datascience #security

## Video
<a href="http://www.youtube.com/watch?feature=player_embedded&v=MEFRie7hmpM" target="_blank"><img src="http://img.youtube.com/vi/MEFRie7hmpM/0.jpg"
alt="What is Machine Learning Security Anyway?" width="50%" /></a>


## Transcript
